\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry} 
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{longtable}
\usepackage{minted}
\usepackage{appendix}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{bookmark}
\usepackage{array}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{booktabs}
\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{Library.bib}

\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

\begin{document}

% ============ TITLE PAGE ============
\title{\huge Data Mining \& Machine Learning F20DL \\ Coursework 2 \\
{\small{\url{https://github.com/friedforfun/SupervisedLearningCW}}}}
\author{Group 4\\Lewis Wilson, Sam Fay-Hunt, Kamil Szymczak, Chun Man }
\date{\today}
\maketitle

% ============ TABLE OF CONTENTS ============
\newpage
\tableofcontents
\thispagestyle{empty}
\pagebreak
\setcounter{page}{1}
% ============ INTRODUCTION ============
\newpage
\section{Introduction}
Based on our experience from Coursework 1 we were aware that many machine learning models would perform poorly with a poorly balanced class distribution, we took measures to resolve this class distribution problem in coursework 1 and this resolved the overfitting issues.
We chose not to use these techniques so we could test the machine learning models with an inadequate dataset, to give us insight into how the different models perform in this situation.

We wrote all experiments in Python 3, using SKlearn because it was easy to implement the models under an interface and pass the discrete models as parameters to “run experiment” functions, we used Tensorflow+Keras for the CNN implementation for ease of use, and speed of training (because of easy integration with Cuda). 
To record and visualise experimental data we used Weights and Biases, this allowed us to perform experiments discretely and share all the results as they were generated.\cite{WeightsBiases}

The purpose of these experiments is to explore how variances in data, learning paradigm and hyperparameters can affect machine learning model performance with respect to overfitting. 
To do this we ran 4 Experiments with each model, covering two machine learning paradigms, we could then compare the behaviour, and try to gain insight into if and why they perform so poorly with data using bad class distributions.
For all but the CNN we recorded the Accuracy, F1, Precision, and Recall metrics, additionally we plotted the confusion matrix, precision-recall curves, ROC curve, class proportions, calibration curve, and learning curve. \cite{WeightsBiases}

The 4 experiments that were run are as follows:
\begin{itemize}
  \item K-fold - We used stratified K-fold because the class label distribution is so unbalanced each of our folds would not be guaranteed to have all classes represented in the training set, this would have unnecessarily skewed our results by testing on never-before-seen data, and would make drawing conclusions much harder. (Figure \ref{StratKFold})
  \item Train-test - Used the provided datasets for training and for each label set a testing vector. 
  \item 4000 and 9000 instances moved from train to test - Same as train-test split above but with the difference being 4000  and 9000 instances moved from train to test. This allowed us to evaluate the model’s performance with different proportions resulting in less training samples.
\end{itemize}

We used the same seed when moving the data from the training set to the testing set, so all experiments for a given classifier used the same training and testing data.

\begin{figure}[H]
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textwidth, keepaspectratio]{Images/StratKFold.png} 
  \caption {Stratified KFolds, Visualised class distributions in each fold (Group)} \label{StratKFold}
\end{figure}

\section{Variation in performance with size of the training and testing sets} \label{sec:VarPerfTrainTestSets}
To investigate the variation in performance we began by looking closely at a single class with a very poor class distribution: “Turn left”.

The turn left dataset has an extremely small number of instances in its training dataset for 9000 moved to testing, Thus we would expect the worst prediction accuracy, in this case, the situation was so bad that our accuracy scores became misleading. Because the representation of ‘turn left’ was so poor in our dataset most of the classifiers simply predicted that there were no instances of turn left in the dataset and recorded a deceiving 97\% accuracy (because 97\% of the dataset is not in fact turn left). It turned out this problem appeared for all binary classifications with the Random Forest classifier and Multilayer perceptron.
\par
The only classifiers that did not exhibit this behaviour were the Logistic Regression (LR) and Convolutional Neural Network classifiers (CNN). CNN scored a staggering 99.9\% accuracy with the same minuscule training data (without out enhancements to reduce overfitting, see Section x), while LR managed 98.3\% with 200 false negatives, 5 false positives and 60 true positives.
\newline
\par
\begin{figure}[H]
  \caption {All class proportions of train (dark blue) to test (light blue)}
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textheight, keepaspectratio]{Images/classProportionsInTargetVariable.png}
\end{figure}
An example distribution of labels between train (dark blue) and test (light blue), for the Beware cycle route ahead class labels, Figures~\ref{fig:turnLeftClassProp},~\ref{fig:turnLeft4k},~\ref{fig:turnLeft9K} show how the proportion of training data moves more substantially for the `no` class, while the `yes' class is so small the change is marginal.


\begin{figure}[H]
  \caption {Beware cycle route ahead class proportions}\label{fig:turnLeftClassProp}
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textheight, keepaspectratio]{Images/Sec1Original.png}
\end{figure}

\begin{figure}[H]
  \caption {Beware cycle route ahead class proportions with 4000 moved to tests}\label{fig:turnLeft4k}
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textheight, keepaspectratio]{Images/Sec1-4k.png}
\end{figure}

\begin{figure}[H]
  \caption {Beware cycle route ahead class proportions with 9000 moved to tests}\label{fig:turnLeft9K}
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textheight, keepaspectratio]{Images/Sec1-9k.png}
\end{figure}

Starting out, we observed the results of J48, the issues are immediately clear, even in the experiment with the full training set J48 is poor at attempting to predict on this kind of data.
When working with `all classes' it is only able to classify the very well represented speed limit signs reliably as speed limit signs, so it manages a high precision at classifying a meta-class of the group of speed limit 30, 50, 60, or 70 signs.
Within this meta-class the accuracy at distinguishing between them is approximately 50\% on average.

\begin{figure}[H]
  \caption {J48 Confusion Matrix}
  \centering 
  \includegraphics[width = \textwidth, height = 0.3\textheight, keepaspectratio]{Images/J48ConfMat.png}
\end{figure}
\FloatBarrier
\par
\par
As the test set grows we can observe that the J48 classifier gets substantially worse at distinguishing between speed limit signs and other signs because the representation of those signs in the dataset gets very low.

With all the classifiers except Logistic Regression and the CNN we see the same story, Multi-Layer perceptron and random forest particularly suffer from this poor class representation issue in the data. We can see in Figure \ref{MLPConfMat} that the Multi-Layer perceptron assigns most labels for 3 classes: Speed limit 30, 50, 70. It also assigns a small number of labels for speed limit 20. So in this baseline test it is already showing overfitting behaviour.

In the test where 9000 instances are moved the overfitting behaviour gets much worse, the Multi-Layer perceptron classifies everything as either speed limit 30 or speed limit 70.


\begin{figure}[h]
  \caption {Multilayer Perceptron Confusion Matrix} \label{MLPConfMat}
  \centering 
  \includegraphics[width = \textwidth, height = 0.3\textheight, keepaspectratio]{Images/Sec1-ConfMat2.png}
\end{figure}



\newpage
\section{Variation in performance with the change in the learning paradigm}
\begin{itemize}
  \item Decision trees performed comparatively poorly, particularly when the training data was reduced.
  \item Multilayer Perceptron had overfitting issues, Logistic Regression was the best, aside from the CNN
\end{itemize}
We have found decision trees performed very poorly particularly when the training data was reduced and also so did the MultiLayer Perceptron which had overfitting issues. Both of these classifiers had 0 recall and 0 precision for predicting individual classes datasets. This means they have not predicted correctly a single sign and should not be used in prediction road signs from images. Our results perfectly support the weak points about decision trees such as that there are variable relationships that decision trees just can’t learn and they are unstable as a small change in the data can lead to a large change in the structure of the optimal decision tree \cite{zhaoComparisonDecisionTree2008}.
\par
However, Logistic Regression performed considerably better, it was actually the best performer from all classification techniques we have used and tested, aside from the CNN. 
\par
Logistic Regression has given best results thus it was less vulnerable to poor class distributions and dealt nicely with overfitted data. A benefit of logistic regression is its interpretability as it understands which pixels are important to determinate what class an image belongs to.
\par
Convolutional Neural Networks worked phenomenally as expected, as they are widely used in image classification.

\newpage
\subsection{Convolutional Neural Networks}


  \begin{table}[h]\caption {Shape of custom CNN}
    \centering
    \begin{tabular}{@{}lll@{}} 
    \toprule
    Layer type & Output Shape & Param \# \\ \midrule
    conv2D & (None, 48, 48, 75) & 750 \\ \midrule
    BatchNormalization & (None, 48, 48, 75) & 300 \\ \midrule
    MaxPooling2D & (None, 24, 24, 75) & 0 \\ \midrule
    Conv2D & (None, 24, 24, 50) & 33800 \\ \midrule
    Dropout & (None, 24, 24, 50) & 0 \\ \midrule
    BatchNormalization & (None, 24, 24, 50) & 200 \\ \midrule
    MaxPooling2D & (None, 12, 12, 50) & 0 \\ \midrule
    Conv2D & (None, 12, 12, 25) & 11275 \\ \midrule
    BatchNormalization & (None, 12, 12, 25) & 100 \\ \midrule
    MaxPooling2D & (None, 6, 6, 25) & 0 \\ \midrule
    Flatten & (None, 900) & 0 \\ \midrule
    Dense & (None, 512) & 461312 \\ \midrule
    Dense & (None, 512) & 262656 \\ \midrule
    Dropout & (None, 512) & 0 \\ \midrule
    Dense & (None, 10) & 5130 \\ \bottomrule
    \end{tabular}
  \end{table}


\subsubsection{Critical limitation of CNN experimental methodology:}
We were unable to disable memoization between experiments over each loop, so all the parameters are restored between runs within a given experiment, consequently, we noted only significant changes to the loss when training on all classes, after which convergence was achieved in a single epoch. We attempted to clear the model parameters, clear the GPU memory, and model saves the randomly initialised parameters and load them back in between experiments but could not prevent the CNN model using data encoded from previous experiments. This does invalidate our results when compared to the discrete experiments in the previous sections, however, this can be a demonstrable advantage of a CNN, in this situation it consistently correctly classifies the images despite changing the dimensions of the output layer and the classification targets (albeit on the same dataset).
\par
\subsubsection{Summary of CNN experimental results}

During all experiments with training and testing data, at worst our CNN scored an accuracy of 97\% on the test dataset when classifying all classes, the CNN was capable of correct classification even when 9000 instances were moved from the training set to the testing set.

The CNN was unsurprisingly the best performing classifier, In many of the k-fold experiments the CNN managed an accuracy of 100\%, but this could be due in part to the memoization effect between experiments, this is particularly noticeable on the k-fold experiments.

The reason we expected the best results for the CNN is that we took measures to reduce overfitting, first, we used 2 dropout layers to reduce overfitting, by regularizing the model we try and encourage each neuron to be more independently useful.
Using the Keras Dataflow class, we produced an augmented dataset, this allowed us to increase the representation of the least represented sign classes, by providing random variance within a zoom, rotation and shift range, effectively providing an infinite stream of data to train with.

For further experiments, it would be interesting to investigate the effects of changing the parameters of the ImageDataGenerator class to train with a wider variety of data.

From Figure \ref{CNNResults}, we can see that using an augmented dataset reduces overfitting and delays the convergence epoch to later in the training process.
\begin{figure}[H]
  \centering 
  \includegraphics[width = 0.8\textwidth, keepaspectratio]{Images/CNN_resultsWithBaseTestData.png} 
  \caption {CNN results with baseline testing data, accuracy is training accuracy, and val\_accuracy is testing accuracy.} \label{CNNResults}
\end{figure}


\newpage
\section{Methodology for Optimising hyperparameters}

Our hyperparameter optimization used Bayesian Optimization \cite{Configuration}. This gaussian process models the function and then chooses parameters to optimize by determining the probability of improvement according to the accuracy metric. 
\par
We opted to use the ‘all classes’ dataset for these sweeps because in our preliminary experiments we noticed the worst prediction accuracy with 10 classes.

Figure \ref{MLPLineGraph} shows a high-level overview of the MLP sweep, dark blue lines indicate lower accuracy and yellow had the highest accuracy. (See Section \ref{MLPSection})

\begin{figure}[h]
  \caption {Multilayer Perceptron} \label{MLPLineGraph}
  \centering 
  \includegraphics[width = \textwidth, height = 0.3\textheight, keepaspectratio]{Images/MLP ParallelCoordGraph.png}
\end{figure}


For the evaluation of our hyperparameters, we gathered data on the linear correlation between each hyperparameter and their accuracy. The feature importance is calculated from training a random forest with hyperparameters and its metrics. \cite{ParameterImportance}
\par
The feature importance metric is useful to identify the features that have a larger impact on the output of the model, coupled with correlation we can find the direction of the relationship. It may be the case that a very important feature is extremely bad for the result, in this case, we will see a strong negative correlation, this information helps us hone in on the best hyperparameters to use to maximise accuracy.
\par
Using the accuracy metric however, caused the models to instead maximise the hyperparameters tendency to overfit this very imbalanced dataset.
\newpage
\section{Variation in performance with varying learning parameters in Decision Trees}
% ===================================== J48 =====================================

\subsection{J48}

Discussed below is the hyperparameter \cite{SklearnTreeDecisionTreeClassifier} importance concerning the performance of J48 trees:

\textbf{max\_depth -} The maximum depth of the tree. \\
\textbf{min\_impurity\_decrease -} Splits a node if this split induces a decrease of the impurity greater than or equal to this value. \\
\textbf{min\_samples\_leaf -} the minimum number of samples to be considered a leaf. \\
\textbf{min\_weight\_fraction\_leaf -} The minimum number of samples required to be at a leaf node. \\
\textbf{criterion\_value -}[‘Gini’,’Entropy’]: Measures the quality of the split.\\
\textbf{max\_features -} [‘auto’,’log2’,’sqrt’] : max features considers the number of features when looking for the best split. Auto just picks the best result so had the same result as log2. \\
\textbf{splitter -} [‘best’, ‘random’]: The strategy used to choose the split at each node. 
\newline

\begin{table}[h]
  \centering
  \begin{tabular}{|p{0.25\linewidth} | p{0.15 \linewidth} | p{0.45\linewidth}|} 
    \hline
    \textbf{Parameter}  & \textbf{Type} &\textbf{Conclusion} \\ \hline
    max\_depth & int & Provided low importance value of 0.012 and provided some correlation 0.084. \\ \hline
    min\_impurity\_decrease & float & Small importance value of 0.051 and a strong negative correlation of -0.241.  \\ \hline
    min\_samples\_leaf & int or float & Low importance of 0.014 and provided a tiny positive correlation of 0.007. \\ \hline
    min\_weight\_fraction\_leaf & float & Gives a strong negative correlation in terms of accuracy, meaning the higher the min\_weight\_fraction\_leaf value, the lower the accuracy.\\ \hline
    criterion & string & Both parameters provided a very low importance value of 0.003. Entropy had a tiny negative correlation of -0.022 and Gini a positive correlation of 0.022. \\ \hline
    max\_features & string & Log2 provided an importance value of 0.133 and a high negative correlation of -0.372. Log2 provided tiny importance of 0.003 but a relatively good correlation of 0.196. \\ \hline
    splitter & string & 'best' had an importance of 0.048 and random 0.044. 'best' has a positive correlation of 0.051 whereas random -0.051. \\ \hline
  \end{tabular}
\end{table}\label{RF_Analysis_Table}
See Parameter Importance (Figure \ref{J48_ParamImp1})
\FloatBarrier
% ===================================== RANDOM FOREST =====================================

\newpage
\subsection{Random Forest}
Discussed below is the hyperparameter \cite{SklearnEnsembleRandomForestClassifier} importance concerning the performance of Random Forest trees:

\par
\textbf{min\_samples\_split -} The minimum number of samples required to split an internal node \\
\textbf{min\_samples\_leaf -} The minimum number of samples required to be at a leaf node.  \\
\textbf{n\_estimators -} The number of trees in the forest.\\
\textbf{min\_weight\_fraction\_leaf -} The minimum number of samples required to be at a leaf node. \\
\textbf{max\_features -} The number of features to consider when looking for the best \\
\textbf{criterion -} The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \\


\begin{table}[ht]
  \centering
  \begin{tabular}{|p{0.25\linewidth} | p{0.15 \linewidth} | p{0.45\linewidth}|} 
    \hline
    \textbf{Parameter}  & \textbf{Type} & \textbf{Conclusion} \\ \hline
    max\_features & string & Contains the options "auto", "sqrt" and "log2". We discovered that "sqrt" has a higher accuracy overall, the accuracy of "log2" varies between the lower end and the median accuracy value.\\ \hline
    min\_samples\_split & int or float & The minimum samples required to split a node has very little impact on accuracy.  \\ \hline
    criterion & string & Gives a perfect negative correlation with respect to accuracy. Correlation values being [Gini = -0.404 ], [Entropy = 0.404 ]. \\ \hline
    n\_estimators & int & This is defined as the number of trees in the forest, it seems to have very little correlation but high importance. \\ \hline
    min\_samples\_leaf & int or float & Gives a strong negative correlation in terms of accuracy, meaning the higher minimum samples at a leaf node, the lower the accuracy. \\ \hline
    min\_weight\_fraction\_leaf & float & Has a somewhat positive correlation to accuracy. e.g. total weight required at a leaf node varies between 76\% and 89\% accuracy\\ \hline
  \end{tabular}
\end{table}\label{RF_Analysis_Table}
See Parameter Importance (Figure \ref{RF_ParamImp1})

\newpage
\section{Variation in performance with varying learning parameters in Neural Networks}
% ===================================== LINEAR CLASSIFIER =====================================
\subsection{Linear Classifier}

For a linear classifier, we have decided to use logistic regression. \cite{SklearnLinearModel}
Shown below is the hyperparameter importance concerning the performance of Logistic Regression.

The accuracy fluctuates depending on which hyperparameters are used. Logistic Regression has a ‘solver’ hyperparameter in sklearn which is the algorithm to use in the optimization. We have tested the following: ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’. Sag and Saga give the best results with 89\% accuracy. Liblinear isn't far behind with 88%.
Other solvers have a negative impact on the performance, this can be seen in the Solver parameter importance table where their correlation values are negative.

Additionally ‘sag’ and ‘saga’ guarantee fast convergence on features with approximately the same scale [R41] which is the case with our datasets as pixel values have the same scale. This can be seen as the number of iterations between 200-800 don’t change much in the accuracy and from the scatter chart of the accuracy of the Logistic Regression vs the number of sweeps created.

\begin{table}[ht]
  \centering
  \begin{tabular}{|p{0.25\linewidth} | p{0.15 \linewidth} | p{0.45\linewidth}|} 
    \hline
    \textbf{Parameter} & \textbf{Type} & \textbf{Conclusion} \\ \hline
    C & float & The smaller this value is the stronger the regularization. For this we settled on 0.08027. Although this parameter did not make a big d ifference and provided similar results for values between 0.8 and 1.2 \\ \hline
    fit\_intercept & bool & From testing we have found out that for this dataset adding a bias works better than not having bias thus True  \\ \hline
    max\_iter & int &  We settled on 342 maximum number of iterations taken for the solver to converge. This parameter does not have a big importance but we found 342 gives accurate results. \\ \hline
    solver & string & Contains the options ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’, default=’lbfgs’. ‘Saga’ gives the best accuracy overall. Solver is the most influential hyperparameter as we can see from the importance and correlation table \\ \hline
    tol & float & tolerance for stopping criteria which tells the algorithm to stop searching when some tolerance is achieved. This parameter did not make a difference, we settled on 0.0002277 \\ \hline


  \end{tabular}
\end{table}\label{LC_Analysis_Table}
See Parameter Importance (Figure \ref{LC_ParamImp1})

% ===================================== MULTILAYER PERCEPTRON =====================================
\newpage
\subsection{Multilayer Perceptron}

\par
\label{MLPSection}
Our sweep on MLP \cite{SklearnNeuralNetwork} indicated that the relu activation function provided the highest accuracy metric for all classes, our search computed this as the most important metric.
However, using this we observed severe overfitting on the speed limit signs in all experiments.

\textbf{activation -} Activation function for the hidden layer. \\
\textbf{alpha -} L2 penalty (regularization term) parameter. \\
\textbf{hidden\_layer\_sizes -} Number of neurons in each hidden layer. \\
\textbf{max\_iter -} Maximum number of iterations. The solver iterates until convergence \\
\textbf{solver -} The solver for weight optimization.\\
\textbf{learning\_rate -} The learning rate controls how quickly the model is adapted to the problem. \\


\begin{table}[ht]
  \centering
  \begin{tabular}{|p{0.25\linewidth} | p{0.15 \linewidth} | p{0.45\linewidth}|} 
    \hline
    \textbf{Parameter} & \textbf{Type} & \textbf{Conclusion} \\ \hline
    alpha & float & When alpha increases the loss increases therefore we went with a relatively low number of 0.0005508. \\ \hline 
    solver & string & ‘lbfgs’ solver did provide the best accuracy consistently when compared to other solvers such as ‘adam’ and ‘sgd’ which supports the statement from \cite{SklearnNeuralNetwork} staying that ‘Lbfgs’ works best for small datasets. \\ \hline
    max\_iter & int & Maximum number of iterations. We used 520 as it gives good results but that is the max so it converges earlier, as long as a value above 200 is used will give high accuracy. \\ \hline
    activation & string & Contains the options: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’. We found relu worked best for our use case  \\ \hline  
    learning\_rate & string & 'adaptive' achieves the highest accuracy while, 'constant' and 'invscaling' vary widely. \\ \hline  
    hidden\_layer\_sizes & tuple & 337 layers worked well for us. But this parameter is not very important as long at its above a fairly high value such as 200. \\ \hline
  \end{tabular}
\end{table}\label{MLP_Analysis_Table}
\FloatBarrier
See Parameter Importance (Figure \ref{MLP_ParamImp1})

\subsection{Further experiments}

We noticed extreme overfitting when working with most classifiers after running our hyperparameter search, next we decided to run a new search, using one of our worst performers, the Multi-Layer perceptron because we felt it had the most room for improvement. Therefore we decided to experiment with it further by running a more complex sweep.
To do this we changed the optimisation metric to minimize loss and randomly varied the classification labels throughout the search, our goal was to see if we could find a single set of parameters that resulted in a better recall, precision, f1, accuracy and loss.

\subsubsection{Turn left label}
Looking closer at the turn left label results a few patterns emerge, the hidden layer size was awarded the highest importance with a low positive correlation to a lower loss, and the learning rate hyperparameter set to constant had the highest overall feature importance with a negative correlation to loss of -0.301.


\subsubsection{Beware Pedestrian Crossing - Looking closer}
We found a few instances in the sweep providing a Precision and Recall Score of 1 \ref{MLPPedCrossing}. The first observation we made concerned the solver hyperparameter. The majority of these results occurred with the ‘lbfgs’ solver, however, ‘sgd’ was able to achieve these recall and precision metrics also but with a much higher loss between 0.03 and 1.4, whereas ‘lbfgs’ consistently had a loss of less than 0.0016.
The variance in loss when the ‘identity’ activation function was used was extreme, from 0.00009 to 2.28e+22, this indicates this activation would be an extremely unreliable choice. 


\begin{figure}[H]
  \centering 
  \includegraphics[width = \textwidth, height = 0.25\textwidth, keepaspectratio]{Images/MLPPedCrossingTable.png} 
  \includegraphics[width = \textwidth, height = 0.4\textwidth, keepaspectratio]{Images/MLPedCrossingGraph.png} 
  \caption {MLP Pedestrian Crossing, table of hyperparameter sweep results with corresponding line chart} \label{MLPPedCrossing}
\end{figure}
\FloatBarrier

During the experiment, we noticed the worst predictions with 0 precision and recall (See Fig \ref{MLPSweepResults}) that MLP results almost always use either ‘adam’ or ‘sgd’ solver and tend to iterate around 20 times before terminating in most cases. We suspect this due to the algorithm being stuck in local maxima and the convergence conditions are satisfied, thus the algorithm terminates early. We observed that the majority of class labels where this occurs are speed signs. 

\begin{figure}[H]
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textwidth, keepaspectratio]{Images/MLPSweepResults-precAndRecall.png} 
  \caption {MLP sweep results - grouping by class label where precision \& recall are 0.} \label{MLPSweepResults}
\end{figure}
\FloatBarrier

\subsubsection{Conclusions about the extended MLP sweep}

Overall, the experiment made it clear that each test set would benefit from individual hyperparameter profiles customised to the data, the only notable correlation with a minimized loss we observed was the ‘lbfgs’ solver, with a positive correlation for recall and precision scores of 0.317, otherwise all metrics demonstrated correlations between -0.2 and 0.078 \ref{MLPSweepAccuracy}.

\begin{figure}[H]
  \centering 
  \includegraphics[width = \textwidth, height = 0.4\textwidth, keepaspectratio]{Images/MLPParamImp-Recall.png} 
  \caption {MLP - Parameter importance with respect to Recall score} \label{MLPSweepAccuracy}
\end{figure}



% ===============================================================

\newpage
\section{Variation in performance according to different metrics}
\subsection{J48 Experiments}

J48 with default test set cycle route ahead binary classification gave a very high precision (~0.95) and a really low recall (0.2) 
Precision shows the ratio of true positives to all the positives, therefore in our use case we have a ratio of signs that the classifier classified correctly to the total the classifier thought are positive. 
\par
Using cycle route ahead dataset as an example, for all signs that are actually cycle route ahead signs the recall tells us how many the classifier classified as cycle route ahead  signs. 
Therefore J48 is very picky and doesn't think many signs are cycle route ahead signs. Pretty much all signs it thinks are cycle route ahead signs are indeed cycle route ahead signs. However, it also misses a lot of cycle route ahead signs, because it is very picky.
\par
With the default test set, J48 provides a precision, recall and f1 scores of 0 for the right turn, left turn, beware pedestrian crossing and speed limit 20 classes the reason is due to 0 true positive classifications. When we investigate why this is for the beware pedestrian crossing class, we noticed the data has a heavy skew of class proportions towards ‘No’ with very few instances of ‘Yes’ found. In fact, this is the case for all of the class labels with scores of 0 for precision, recall and f1.

\subsection{Random Forest \& MLP}
Both random forest and MLP suffered overfit issues particularly with binary class labels; they scored 0 for precision, recall, and F1 in all experiments. We hypothesize this is caused by the same reasons described in Section \ref{sec:VarPerfTrainTestSets}. Likewise, all classes perform poorly the model classifies every class as exclusively either speed limit 30, 50, and 70 in the training and testing data sets, the k-fold experiments simply classifies all classes as speed limit 50; the most well-represented class in our dataset. 

\subsection{Logistic Regresssion}
Logistic Regression is excellent at predicting classes for the dataset, which contains all signs, which is not easy to do. Logistic Regression has relatively good precision and recall when compared to other classifiers with 89 for both. This means it is very good at predicting all different signs (See Figure \ref{LogRegConfMat}).

\begin{figure}
  \centering
  \includegraphics [width = \textwidth, height = 0.5\textheight, keepaspectratio]{Images/LRConfMat.png} \caption{Logistic regression confusion matrix all classes} \label{LogRegConfMat}
\end{figure}
% ============ APPENDICES BEGINNING ============================================
\pagebreak
\appendix
\appendixpage
\addappheadtotoc
%\begin{appendices}

\section{Workload}

% =================== Workload Split Table ===================

\subsection{Workload split}
  
  \begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.2\linewidth} | p{0.75\linewidth}|} 
      \hline
      \textbf{Team member}  & \textbf{Involvement} \\ \hline
      Lewis Wilson & Contributed to the experimenting and analysis of J48, Report writing. \\ \hline
      Chun Man & Contributed to the experimenting and analysis of Random Forest, Report writing. \\ \hline
      Sam Fay-Hunt & Contributed to the report, coded all the automated experiments, weights and biases logging and integration, the interface used by each sklearn model, and the entire CNN. \\ \hline
      Kamil Szymczak & Contributed to the experimenting and analysis of Logistic Regression, Report writing. \\ \hline
    \end{tabular}
  \end{table}\label{ContributionTab}

As a team, we are happy with everyone's contributions to the project. All team members were punctual and showed up to all scheduled meetings. Sam took the lead as project manager throughout the project delegating the workload and providing support to others.

\newpage
\section{References}
\printbibliography

\newpage
\section{Links to experimental results}
The following is a list of the links to all of our experimental data.

\subsection{J48}
\begin{itemize}
  \item \href{https://wandb.ai/supervisedlearning/J48%20all%20Kfolds/?workspace=user-jimmym620}{Kfold}
  \item \href{https://wandb.ai/supervisedlearning/J48%20all%20test%20set%20experiments?workspace=user-dardric}{Baseline test}
  \item \href{https://wandb.ai/supervisedlearning/J48%20all%204000%20moved%20to%20test%20experiments?workspace=user-dardric}{4k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/J48%20all%209000%20moved%20to%20test%20experiments?workspace=user-dardric}{9k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/J48-sweeps/sweeps/iujbgua2?workspace=user-dardric}{Hyperparameter sweep}
\end{itemize}

\subsection{Random Forest}
\begin{itemize}
  \item \href{https://wandb.ai/supervisedlearning/RF%20all%20Kfolds?workspace=user-jimmym620}{Kfold}
  \item \href{https://wandb.ai/supervisedlearning/RF%20all%20test%20set%20experiments?workspace=user-jimmym620}{Baseline test}
  \item \href{https://wandb.ai/supervisedlearning/RF%20all%204000%20moved%20to%20test%20experiments?workspace=user-jimmym620}{4k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/RF%20all%209000%20moved%20to%20test%20experiments?workspace=user-jimmym620}{9k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/RF-sweeps/sweeps/eh36kpyc?workspace=user-jimmym620}{Hyperparameter sweep}
\end{itemize}

\subsection{MLP}
\begin{itemize}
  \item \href{https://wandb.ai/supervisedlearning/MLP%20all%20Kfolds/experiments?}{Kfold}
  \item \href{https://wandb.ai/supervisedlearning/MLP%20all%20test%20set%20experiments?}{Baseline test}
  \item \href{https://wandb.ai/supervisedlearning/MLP%20all%204000%20moved%20to%20test%20experiments?}{4k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/MLP%20all%209000%20moved%20to%20test%20experiments?}{9k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/MLP-sweeps/sweeps/aum1y9nw?workspace=user-samfh}{Initial hyperparameter sweep}
  \item \href{https://wandb.ai/supervisedlearning/MLP-sweeps/sweeps/ha5qixd1?workspace=user-samfh}{Enhanced hyperparameter sweep}
\end{itemize}

\subsection{LR}
\begin{itemize}
  \item \href{https://wandb.ai/supervisedlearning/LR%20all%20Kfolds?workspace=user-dardric}{Kfold}
  \item \href{https://wandb.ai/supervisedlearning/LR%20all%20test%20set%20experiments?workspace=user-dardric}{Baseline test}
  \item \href{https://wandb.ai/supervisedlearning/LR%20all%204000%20moved%20to%20test%20experiments?workspace=user-dardric}{4k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/LR%20all%209000%20moved%20to%20test%20experiments?workspace=user-dardric}{9k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/LC-sweeps/sweeps/409bnuc6?workspace=user-dardric}{Hyperparameter sweep}
\end{itemize}

\subsection{CNN}
\begin{itemize}
  \item \href{https://wandb.ai/supervisedlearning/CNN%20All%20kf%20experiments/table?workspace=user-samfh}{Kfold}
  \item \href{https://wandb.ai/supervisedlearning/CNN%20all%20test%20set%20experiments?workspace=user-samfh }{Baseline test}
  \item \href{https://wandb.ai/supervisedlearning/CNN%20all%204000%20moved%20to%20test%20experiments?workspace=user-samfh}{4k instances moved to test}
  \item \href{https://wandb.ai/supervisedlearning/CNN%20all%209000%20moved%20to%20test%20experiments?workspace=user-samfh}{9k instances moved to test}

\end{itemize}

% =================== J48 APPENDIX here ===================
\newpage
\section{J48}

\subsection{J48 Parameter Importance}
  \begin{table}[ht]\caption{J48 Parameter metrics}
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
        min\_weight\_fraction\_leaf & 0.668  & -0.666 \\ \hline
        min\_impurity\_decrease & 0.052 & -0.241 \\ \hline
        min\_samples\_leaf & 0.016 & 0.007 \\ \hline
        max\_depth & 0.012 & 0.084 \\ \hline
        min\_samples\_split & 0.009 & -0.108 \\ \hline

    \end{tabular}
  \end{table}\label{J48_ParamImp1}

  \begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
        max\_features.value\_log2 & 0.133 & -0.372 \\ \hline
        max\_features.value\_sqrt & 0.003 & 0.196 \\ \hline
        splitter.value\_best & 0.049 & 0.051 \\ \hline
        splitter.value\_random & 0.043 & -0.051 \\ \hline
        criterion.value\_entropy & 0.002 & -0.022 \\ \hline
        criterion.value\_gini & 0.002 & 0.022 \\ \hline

    \end{tabular}
  \end{table}\label{J48_ParamImp2}

\begin{figure}[h]
  \caption {J48 Parameters} \label{ParallelCoordJ48}
  \centering 
  \includegraphics[width = \textwidth, height = \textwidth, keepaspectratio]{Images/J48 ParallelCoordGraph.png}
\end{figure}



\FloatBarrier
% =================== Random Forest APPENDIX here ===================
\newpage
\section{Random Forest}

\subsection{Random Forest Parameter Importance}
  \begin{table}[ht]\caption{Random Forest Parameter metrics}
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
        min\_samples\_split & 0.005 & 0.306 \\ \hline
        min\_samples\_leaf & 0.375 & -0.725 \\ \hline
        n\_estimators & 0.016 & 0.092 \\ \hline
        min\_weight\_fraction\_leaf & 0.013 & 0.123 \\ \hline
    \end{tabular}
  \end{table}\label{RF_ParamImp1}

  \begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
      max\_features.value\_sqrt & 0.001 & 0.563 \\ \hline
      max\_features.value\_log2 & 0.565 & -0.752 \\ \hline
      criterion.value\_entropy & 0.012 & 0.404 \\ \hline
      criterion.value\_gini & 0.012 & -0.404 \\ \hline
    \end{tabular}
  \end{table}\label{RF_ParamImp2}

\begin{figure}[h]
    \caption {Random Forest Parameters} \label{ParallelCoordRF}
    \centering 
    \includegraphics[width = \textwidth, height = \textwidth, keepaspectratio]{Images/RF ParallelCoordGraph.png}
\end{figure}


  
  \FloatBarrier
% =================== Linear Classifier APPENDIX here ===================
\newpage
\section{Linear Classifier}

\subsection{Linear Classifier Parameter Importance}
  \begin{table}[ht]\caption{Linear Classifier Parameter metrics}
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Solver Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
      lbfgs & 0.786 & -0.869 \\ \hline
      newton-cg & 0.159 & -0.271 \\ \hline
      liblinear & 0.042 & -0.037 \\ \hline
      saga & 0.009 & 0.691 \\ \hline
      sag & 0.003 & 0.170 \\ \hline
    \end{tabular}
  \end{table}\label{LC_ParamImp1}

  \begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Config Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
      max-iter & 0.194 & -0.634 \\ \hline
      l1-ratio & 0.124 & -0.510 \\ \hline
      fit-intercept & 0.058 & 0.367 \\ \hline
      tol & 0.046 & -0.174 \\ \hline
      C & 0.031 & 0.260 \\ \hline
    \end{tabular}
  \end{table}\label{LC_ParamImp2}

\begin{figure}[h]
    \caption {Random Forest Parameters} \label{ParallelCoordLC}
    \centering 
    \includegraphics[width = \textwidth, height = \textwidth, keepaspectratio]{Images/LC ParallelCoordGraph.png}
\end{figure}

  \FloatBarrier
% =================== Multilayer Perceptron APPENDIX here ===================
\newpage
\section{Multilayer Perceptron}

\subsection{Multilayer Perceptron Parameter Importance}
  \begin{table}[ht]\caption{MLP Parameter metrics}
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
        hidden\_layer\_sizes & 0.095 & -0.101 \\ \hline
        max\_iter & 0.091 & 0.072 \\ \hline
        alpha & 0.061 & 0.171 \\ \hline
    \end{tabular}
  \end{table}\label{MLP_ParamImp1}

  
  \begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.3\linewidth}| p{0.3\linewidth}|} 
      \hline
      \textbf{Parameter Config}  & \textbf{Importance} & \textbf{Correlation} \\ \hline
        solver.value\_lbfgs & 0.530 & 0.728 \\ \hline
        solver.value\_adam & 0.027 & -0.245 \\ \hline
        solver.value\_sgd & 0.024 & -0.640 \\ \hline
        activation.value\_identity & 0.098 & -0.169 \\ \hline
        activation.value\_relu & 0.033 & 0.301 \\ \hline
        activation.value\_tanh & 0.018 & -0.035 \\ \hline
        activation.value\_logistic & 0.006 & -0.270 \\ \hline
        learning\_rate.value\_adaptive & 0.012 & 0.507 \\ \hline
        learning\_rate.value\_constant & 0.004 & -0.442 \\ \hline
        learning\_rate.value\_invscaling & 0.001 & -0.224 \\ \hline
    \end{tabular}
  \end{table}\label{MLP_ParamImp2}

\begin{figure}[h]
  \caption {Multilayer Perceptron Parameters} \label{ParallelCoordMLP}
  \centering 
  \includegraphics[width = \textwidth, height = \textwidth, keepaspectratio]{Images/MLP ParallelCoordGraph.png}
\end{figure}
\FloatBarrier

\begin{figure}[h]
  \caption {Data with evenly split groups} \label{DataWWithEvenlySplitGroups}
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textwidth, keepaspectratio]{Images/DataWithEvenlySplitGroups.png}
\end{figure}


\begin{figure}[h]
  \caption {KFold} \label{KFoldVisualised}
  \centering 
  \includegraphics[width = \textwidth, height = 0.5\textwidth, keepaspectratio]{Images/KFold.png}
\end{figure}


\FloatBarrier
% =================== END APPENDIX ===================
%\end{appendices}


\end{document}
